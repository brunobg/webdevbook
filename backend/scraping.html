<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Scraping and crawlers | A full stack approach to integrated web and native app development</title>
    <meta name="generator" content="VuePress 1.5.1">
    
    <meta name="description" content="Creating native and web apps using Vue, NativeScript and Laravel">
    <link rel="preload" href="/webdevbook/assets/css/0.styles.ff1b3640.css" as="style"><link rel="preload" href="/webdevbook/assets/js/app.a8843243.js" as="script"><link rel="preload" href="/webdevbook/assets/js/2.7bd00ffb.js" as="script"><link rel="preload" href="/webdevbook/assets/js/19.55d79551.js" as="script"><link rel="prefetch" href="/webdevbook/assets/js/10.e4b20976.js"><link rel="prefetch" href="/webdevbook/assets/js/11.904536a7.js"><link rel="prefetch" href="/webdevbook/assets/js/12.b7a199c6.js"><link rel="prefetch" href="/webdevbook/assets/js/13.91b6d744.js"><link rel="prefetch" href="/webdevbook/assets/js/14.77f13f39.js"><link rel="prefetch" href="/webdevbook/assets/js/15.6028fea7.js"><link rel="prefetch" href="/webdevbook/assets/js/16.5e9aa680.js"><link rel="prefetch" href="/webdevbook/assets/js/17.9b2897ed.js"><link rel="prefetch" href="/webdevbook/assets/js/18.9529298c.js"><link rel="prefetch" href="/webdevbook/assets/js/20.34699ae8.js"><link rel="prefetch" href="/webdevbook/assets/js/21.3e4032d5.js"><link rel="prefetch" href="/webdevbook/assets/js/22.42c0a79c.js"><link rel="prefetch" href="/webdevbook/assets/js/23.4ff89230.js"><link rel="prefetch" href="/webdevbook/assets/js/24.1e98f188.js"><link rel="prefetch" href="/webdevbook/assets/js/25.0c86e95c.js"><link rel="prefetch" href="/webdevbook/assets/js/26.a988a395.js"><link rel="prefetch" href="/webdevbook/assets/js/27.64ae7f3f.js"><link rel="prefetch" href="/webdevbook/assets/js/28.ba190317.js"><link rel="prefetch" href="/webdevbook/assets/js/29.e3afb9ec.js"><link rel="prefetch" href="/webdevbook/assets/js/3.70b562fb.js"><link rel="prefetch" href="/webdevbook/assets/js/30.0c898264.js"><link rel="prefetch" href="/webdevbook/assets/js/31.af4397de.js"><link rel="prefetch" href="/webdevbook/assets/js/32.41bd8b29.js"><link rel="prefetch" href="/webdevbook/assets/js/33.0e4cc2c9.js"><link rel="prefetch" href="/webdevbook/assets/js/34.31e711d9.js"><link rel="prefetch" href="/webdevbook/assets/js/35.8d37751b.js"><link rel="prefetch" href="/webdevbook/assets/js/36.31e0e52d.js"><link rel="prefetch" href="/webdevbook/assets/js/4.09b503c2.js"><link rel="prefetch" href="/webdevbook/assets/js/5.c22a31da.js"><link rel="prefetch" href="/webdevbook/assets/js/6.512d59c2.js"><link rel="prefetch" href="/webdevbook/assets/js/7.53cdc081.js"><link rel="prefetch" href="/webdevbook/assets/js/8.3e7409d1.js"><link rel="prefetch" href="/webdevbook/assets/js/9.2a3e4724.js">
    <link rel="stylesheet" href="/webdevbook/assets/css/0.styles.ff1b3640.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/webdevbook/" class="home-link router-link-active"><!----> <span class="site-name">A full stack approach to integrated web and native app development</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <ul class="sidebar-links"><li><a href="/webdevbook/" aria-current="page" class="sidebar-link">Home</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Frontend</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Backend</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="scraping-and-crawlers"><a href="#scraping-and-crawlers" class="header-anchor">#</a> Scraping and crawlers</h1> <p>This chapter is an aside. It will talk a bit about scraping data from the internet. Maybe you need to pull data from sites that don't have an API, maybe you are migrating from a site that is so awful that there's no way to get structured data from the database itself. This is a short and rough overview of how you can write a simple crawler to do that, and strategies to make it work well.</p> <p>There are many, many services online these days that not only crawl for you, but do data processing too, with a lot of code ready for you in simple web interfaces. It's quite possible that they will be easier and faster to use than writing code.</p> <p>If you are going to follow the code route, <a href="https://docs.scrapy.org" target="_blank" rel="noopener noreferrer">Scrapy<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a well known open source crawler. There is a good <a href="https://docs.scrapy.org/en/latest/intro/tutorial.html" target="_blank" rel="noopener noreferrer">tutorial<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>Creating a crawler is easy:</p> <div class="language-shell extra-class"><pre class="language-shell"><code>$ scrapy startproject mycrawler
New Scrapy project <span class="token string">'mycrawler'</span>, using template directory <span class="token string">'/usr/lib64/python3.6/site-packages/scrapy/templates/project'</span>, created in:
    /home/corollarium/git/Corollarium/vinarium/mycrawler

You can start your first spider with:
    <span class="token builtin class-name">cd</span> mycrawler
    scrapy genspider example example.com
</code></pre></div><p>But we won't get into code here.</p> <h2 id="extracting-data-101"><a href="#extracting-data-101" class="header-anchor">#</a> Extracting data 101</h2> <p>Crawling is the easy part: you download HTML and follow links. How do you extract data?</p> <p>Many sites these days use <a href="https://schema.org" target="_blank" rel="noopener noreferrer">a standard structured data schema<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. This makes them get higher ranking in search engines, so there's a good stimulus for them to use it. Schema.org is quite common these days, and it was designed exactly for atuomatic data retrieval.</p> <p><a href="https://github.com/scrapinghub/extruct" target="_blank" rel="noopener noreferrer">Extruct<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> can be used with Scrapy to easily get structured data.</p> <p>Many times you need to scrap data that is not structured, however. This means that you need to write code that finds the parts of the HTML page that you want, with CSS or XPath expressions. Scrapy has good documentation on that, so here are pointers to designing the expressions themselves.</p> <p>Your browser Developer Tools will be your friend. Open the page you need to crawl there and inspect the elements you want to extract. With a bit of luck they'll have a unique id or a unique class. This makes extracting them trivial and robust. If not, try to build an expression that is simple and robust. <code>div &gt; div &gt; div &gt; ul &gt; li</code> is a bad example: any changes to the layout will break it, or any other HTML that fits this very general pattern will give you false data. Prefer to use classes and ids as much as possible and keep your expressions simple and robust.</p> <h2 id="feeding-data-to-your-backend"><a href="#feeding-data-to-your-backend" class="header-anchor">#</a> Feeding data to your backend</h2> <p>There are a couple of main strategies here. One is to save all crawled data to a file in JSON format and similar. The crawler does not talk to your web backend at all, and you write a small script that loads the JSON and stores it in the database, either through GraphQL/REST requests, or directly using the backend code. Avoid sending it to the database, since you'll lose your backend data validation -- a must from data that comes from external sources.</p> <p>The other is to integrate into your crawler code to send it to the backend. As soon as data is crawled it's sent to your backend. It makes things a little tighter -- your crawler can break because your web server changed something. But you don't need to worry with a third process to link the backend and the crawler.</p> <h3 id="merge-strategies"><a href="#merge-strategies" class="header-anchor">#</a> Merge strategies</h3> <p>Sometimes you are crawling data just once -- to pull data from an older site that is being replaced. But often you'll need to recrawl the same sites and merge it with data you crawled before. This is typical for price comparison websites, for example, or with aggregators that will crawl data that was already parsed before while looking for new data. This means you need a merge strategy.</p> <p>Does your data ever change? If not just check the unicity of a key. The page URL works great as a key. If the key is in your database, skip it. If possible, make your crawler skip it too to avoid consuming useless traffic.</p> <p>Does your data change? Then you need not only a unique key, but the code that access your database needs to be capable of updating it and not only inserting. Upsert operations are your friend here. The most basic idea is to always upsert, overwriting previous data. A bit more involved, but necessary in a few cases, is to create new entries for new data, perhaps only pieces of it. In the case of our price comparison example, that means that you want to store the prices with the dates they were crawled, so you can have a history.</p> <h2 id="testing-and-when-things-break"><a href="#testing-and-when-things-break" class="header-anchor">#</a> Testing and when things break</h2> <p>They say the only certainties in life are death and taxes, but you can pretty much add that crawlers will break as a third one. Since you are fetching data from third parties that you have no control of, some day they will change their website and break your crawler. Also, like any code, crawlers need to be tested. How to ensure they work and how to detect when things break?</p> <p>Like all other software, automatic testing is the best option we have.</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main> <footer data-v-136409f7><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" data-v-136409f7><img alt="Creative Commons License" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" style="border-width: 0;" data-v-136409f7></a> <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type" data-v-136409f7>A full stack approach to integrated web and native app development</span>
  by
  <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName" data-v-136409f7>Bruno Barberi Gnecco</span>
  is licensed under a
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" data-v-136409f7>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
    License</a>.
</footer></div><div class="global-ui"></div></div>
    <script src="/webdevbook/assets/js/app.a8843243.js" defer></script><script src="/webdevbook/assets/js/2.7bd00ffb.js" defer></script><script src="/webdevbook/assets/js/19.55d79551.js" defer></script>
  </body>
</html>
